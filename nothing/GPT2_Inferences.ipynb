{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "paoiLMXfdT0J",
        "outputId": "0166542c-cf0c-4506-f363-1d189b6978b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tiktoken in c:\\users\\thuan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.6.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\thuan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tiktoken) (2022.10.31)\n",
            "Requirement already satisfied: requests>=2.26.0 in c:\\users\\thuan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tiktoken) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\thuan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\thuan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\thuan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2.1.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\thuan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2023.11.17)\n"
          ]
        }
      ],
      "source": [
        "!pip install tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tM8KynRib-S-",
        "outputId": "24341115-cdc8-4898-d72c-2d0e353f5452"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "import tiktoken\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "from transformers import GPT2Model\n",
        "import torch.nn as nn\n",
        "import tiktoken\n",
        "import os\n",
        "import urllib.request\n",
        "import re\n",
        "\n",
        "\n",
        "class GPTDatasetV1(Dataset):\n",
        "    def __init__(self, inp, out):\n",
        "        self.inp = inp\n",
        "        self.tar = out\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.inp)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.inp[idx], self.tar[idx]\n",
        "\n",
        "\n",
        "def text_to_token_ids(text, tokenizer):\n",
        "    encoded = tokenizer.encode(text)\n",
        "    encoded_tensor = torch.tensor(encoded).unsqueeze(0)  # add batch dimension\n",
        "    return encoded_tensor\n",
        "\n",
        "\n",
        "def token_ids_to_text(token_ids, tokenizer):\n",
        "    flat = token_ids.squeeze(0)  # remove batch dimension\n",
        "    return tokenizer.decode(flat.tolist())\n",
        "\n",
        "def generate(model, idx, max_new_tokens, context_size, temperature, top_k=None):\n",
        "\n",
        "    # For-loop is the same as before: Get logits, and only focus on last time step\n",
        "    for _ in range(max_new_tokens):\n",
        "        idx_cond = idx[:, -context_size:]\n",
        "        with torch.no_grad():\n",
        "            logits = model(idx_cond)\n",
        "        logits = logits[:, -1, :]\n",
        "\n",
        "        # New: Filter logits with top_k sampling\n",
        "        if top_k is not None:\n",
        "            # Keep only top_k values\n",
        "            top_logits, _ = torch.topk(logits, top_k)\n",
        "            min_val = top_logits[:, -1]\n",
        "            logits = torch.where(logits < min_val, torch.tensor(\n",
        "                float('-inf')).to(logits.device), logits)\n",
        "\n",
        "        # New: Apply temperature scaling\n",
        "        if temperature > 0.0:\n",
        "            logits = logits / temperature\n",
        "\n",
        "            # Apply softmax to get probabilities\n",
        "            probs = torch.softmax(logits, dim=-1)  # (batch_size, context_len)\n",
        "\n",
        "            # Sample from the distribution\n",
        "            idx_next = torch.multinomial(\n",
        "                probs, num_samples=1)  # (batch_size, 1)\n",
        "\n",
        "        # Otherwise same as before: get idx of the vocab entry with the highest logits value\n",
        "        else:\n",
        "            idx_next = torch.argmax(\n",
        "                logits, dim=-1, keepdim=True)  # (batch_size, 1)\n",
        "\n",
        "        # Same as before: append sampled index to the running sequence\n",
        "        idx = torch.cat((idx, idx_next), dim=1)  # (batch_size, num_tokens+1)\n",
        "\n",
        "    return idx\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_in, d_out, block_size, dropout, num_heads, qkv_bias=False):\n",
        "        super().__init__()\n",
        "        assert d_out % num_heads == 0, \"d_out must be divisible by n_heads\"\n",
        "\n",
        "        self.d_out = d_out\n",
        "        self.num_heads = num_heads\n",
        "        # Reduce the projection dim to match desired output dim\n",
        "        self.head_dim = d_out // num_heads\n",
        "\n",
        "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        # Linear layer to combine head outputs\n",
        "        self.out_proj = nn.Linear(d_out, d_out)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.register_buffer('mask', torch.triu(\n",
        "            torch.ones(block_size, block_size), diagonal=1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, num_tokens, d_in = x.shape\n",
        "\n",
        "        keys = self.W_key(x)  # Shape: (b, num_tokens, d_out)\n",
        "        queries = self.W_query(x)\n",
        "        values = self.W_value(x)\n",
        "\n",
        "        # We implicitly split the matrix by adding a `num_heads` dimension\n",
        "        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n",
        "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "\n",
        "        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n",
        "        keys = keys.transpose(1, 2)\n",
        "        queries = queries.transpose(1, 2)\n",
        "        values = values.transpose(1, 2)\n",
        "\n",
        "        # Compute scaled dot-product attention (aka self-attention) with a causal mask\n",
        "        # Dot product for each head\n",
        "        attn_scores = queries @ keys.transpose(2, 3)\n",
        "\n",
        "        # Original mask truncated to the number of tokens and converted to boolean\n",
        "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
        "\n",
        "        # Use the mask to fill attention scores\n",
        "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
        "\n",
        "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
        "        attn_weights = self.dropout(attn_weights)\n",
        "\n",
        "        # Shape: (b, num_tokens, num_heads, head_dim)\n",
        "        context_vec = (attn_weights @ values).transpose(1, 2)\n",
        "\n",
        "        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n",
        "        context_vec = context_vec.reshape(b, num_tokens, self.d_out)\n",
        "        context_vec = self.out_proj(context_vec)  # optional projection\n",
        "\n",
        "        return context_vec\n",
        "\n",
        "\n",
        "class LayerNorm(nn.Module):\n",
        "    def __init__(self, emb_dim):\n",
        "        super().__init__()\n",
        "        self.eps = 1e-5\n",
        "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
        "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = x.mean(dim=-1, keepdim=True)\n",
        "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
        "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
        "        return self.scale * norm_x + self.shift\n",
        "\n",
        "\n",
        "class GELU(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return 0.5 * x * (1 + torch.tanh(\n",
        "            torch.sqrt(torch.tensor(2.0 / torch.pi)) *\n",
        "            (x + 0.044715 * torch.pow(x, 3))\n",
        "        ))\n",
        "\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n",
        "            GELU(),\n",
        "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]),\n",
        "            nn.Dropout(cfg[\"drop_rate\"])\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)\n",
        "\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.att = MultiHeadAttention(\n",
        "            d_in=cfg[\"emb_dim\"],\n",
        "            d_out=cfg[\"emb_dim\"],\n",
        "            block_size=cfg[\"ctx_len\"],\n",
        "            num_heads=cfg[\"n_heads\"],\n",
        "            dropout=cfg[\"drop_rate\"],\n",
        "            qkv_bias=cfg[\"qkv_bias\"])\n",
        "        self.ff = FeedForward(cfg)\n",
        "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
        "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
        "        self.drop_resid = nn.Dropout(cfg[\"drop_rate\"])\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Shortcut connection for attention block\n",
        "        shortcut = x\n",
        "        x = self.norm1(x)\n",
        "        x = self.att(x)   # Shape [batch_size, num_tokens, emb_size]\n",
        "        x = self.drop_resid(x)\n",
        "        x = x + shortcut  # Add the original input back\n",
        "\n",
        "        # Shortcut connection for feed-forward block\n",
        "        shortcut = x\n",
        "        x = self.norm2(x)\n",
        "        x = self.ff(x)\n",
        "        x = self.drop_resid(x)\n",
        "        x = x + shortcut  # Add the original input back\n",
        "\n",
        "        return x\n",
        "\n",
        "# input (BS, SEQ_LEN)\n",
        "\n",
        "\n",
        "class GPTModel(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
        "        self.pos_emb = nn.Embedding(cfg[\"ctx_len\"], cfg[\"emb_dim\"])\n",
        "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
        "\n",
        "        self.trf_blocks = nn.Sequential(\n",
        "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
        "\n",
        "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
        "        self.out_head = nn.Linear(\n",
        "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False)\n",
        "\n",
        "    def forward(self, in_idx):\n",
        "        batch_size, seq_len = in_idx.shape\n",
        "        tok_embeds = self.tok_emb(in_idx)\n",
        "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
        "        x = tok_embeds + pos_embeds  # Shape [batch_size, num_tokens, emb_size]\n",
        "        x = self.drop_emb(x)\n",
        "        x = self.trf_blocks(x)\n",
        "        x = self.final_norm(x)\n",
        "        logits = self.out_head(x)\n",
        "        return logits\n",
        "\n",
        "\n",
        "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
        "    # idx is (B, T) array of indices in the current context\n",
        "    for _ in range(max_new_tokens):\n",
        "\n",
        "        # Crop current context if it exceeds the supported context size\n",
        "        # E.g., if LLM supports only 5 tokens, and the context size is 10\n",
        "        # then only the last 5 tokens are used as context\n",
        "        idx_cond = idx[:, -context_size:]\n",
        "\n",
        "        # Get the predictions\n",
        "        with torch.no_grad():\n",
        "            logits = model(idx_cond)\n",
        "\n",
        "        # Focus only on the last time step\n",
        "        # (batch, n_token, vocab_size) becomes (batch, vocab_size)\n",
        "        logits = logits[:, -1, :]\n",
        "\n",
        "        # Get the idx of the vocab entry with the highest logits value\n",
        "        idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch, 1)\n",
        "\n",
        "        # Append sampled index to the running sequence\n",
        "        idx = torch.cat((idx, idx_next), dim=1)  # (batch, n_tokens+1)\n",
        "\n",
        "    return idx\n",
        "\n",
        "\n",
        "def assign_check(left, right):\n",
        "    if left.shape != right.shape:\n",
        "        raise ValueError(\n",
        "            f\"Shape mismatch. Left: {left.shape}, Right: {right.shape}\")\n",
        "    return torch.nn.Parameter(torch.tensor(right))\n",
        "\n",
        "    import numpy as np\n",
        "\n",
        "\n",
        "def load_weights(gpt, gpt_hf):\n",
        "\n",
        "    d = gpt_hf.state_dict()\n",
        "\n",
        "    gpt.pos_emb.weight = assign_check(gpt.pos_emb.weight, d[\"wpe.weight\"])\n",
        "    gpt.tok_emb.weight = assign_check(gpt.tok_emb.weight, d[\"wte.weight\"])\n",
        "\n",
        "    for b in range(BASE_CONFIG[\"n_layers\"]):\n",
        "        q_w, k_w, v_w = np.split(d[f\"h.{b}.attn.c_attn.weight\"], 3, axis=-1)\n",
        "        gpt.trf_blocks[b].att.W_query.weight = assign_check(\n",
        "            gpt.trf_blocks[b].att.W_query.weight, q_w.T)\n",
        "        gpt.trf_blocks[b].att.W_key.weight = assign_check(\n",
        "            gpt.trf_blocks[b].att.W_key.weight, k_w.T)\n",
        "        gpt.trf_blocks[b].att.W_value.weight = assign_check(\n",
        "            gpt.trf_blocks[b].att.W_value.weight, v_w.T)\n",
        "\n",
        "        q_b, k_b, v_b = np.split(d[f\"h.{b}.attn.c_attn.bias\"], 3, axis=-1)\n",
        "        gpt.trf_blocks[b].att.W_query.bias = assign_check(\n",
        "            gpt.trf_blocks[b].att.W_query.bias, q_b)\n",
        "        gpt.trf_blocks[b].att.W_key.bias = assign_check(\n",
        "            gpt.trf_blocks[b].att.W_key.bias, k_b)\n",
        "        gpt.trf_blocks[b].att.W_value.bias = assign_check(\n",
        "            gpt.trf_blocks[b].att.W_value.bias, v_b)\n",
        "\n",
        "        gpt.trf_blocks[b].att.out_proj.weight = assign_check(\n",
        "            gpt.trf_blocks[b].att.out_proj.weight, d[f\"h.{b}.attn.c_proj.weight\"].T)\n",
        "        gpt.trf_blocks[b].att.out_proj.bias = assign_check(\n",
        "            gpt.trf_blocks[b].att.out_proj.bias, d[f\"h.{b}.attn.c_proj.bias\"])\n",
        "\n",
        "        gpt.trf_blocks[b].ff.layers[0].weight = assign_check(\n",
        "            gpt.trf_blocks[b].ff.layers[0].weight, d[f\"h.{b}.mlp.c_fc.weight\"].T)\n",
        "        gpt.trf_blocks[b].ff.layers[0].bias = assign_check(\n",
        "            gpt.trf_blocks[b].ff.layers[0].bias, d[f\"h.{b}.mlp.c_fc.bias\"])\n",
        "        gpt.trf_blocks[b].ff.layers[2].weight = assign_check(\n",
        "            gpt.trf_blocks[b].ff.layers[2].weight, d[f\"h.{b}.mlp.c_proj.weight\"].T)\n",
        "        gpt.trf_blocks[b].ff.layers[2].bias = assign_check(\n",
        "            gpt.trf_blocks[b].ff.layers[2].bias, d[f\"h.{b}.mlp.c_proj.bias\"])\n",
        "\n",
        "        gpt.trf_blocks[b].norm1.scale = assign_check(\n",
        "            gpt.trf_blocks[b].norm1.scale, d[f\"h.{b}.ln_1.weight\"])\n",
        "        gpt.trf_blocks[b].norm1.shift = assign_check(\n",
        "            gpt.trf_blocks[b].norm1.shift, d[f\"h.{b}.ln_1.bias\"])\n",
        "        gpt.trf_blocks[b].norm2.scale = assign_check(\n",
        "            gpt.trf_blocks[b].norm2.scale, d[f\"h.{b}.ln_2.weight\"])\n",
        "        gpt.trf_blocks[b].norm2.shift = assign_check(\n",
        "            gpt.trf_blocks[b].norm2.shift, d[f\"h.{b}.ln_2.bias\"])\n",
        "\n",
        "        gpt.final_norm.scale = assign_check(\n",
        "            gpt.final_norm.scale, d[f\"ln_f.weight\"])\n",
        "        gpt.final_norm.shift = assign_check(\n",
        "            gpt.final_norm.shift, d[f\"ln_f.bias\"])\n",
        "        gpt.out_head.weight = assign_check(\n",
        "            gpt.out_head.weight, d[\"wte.weight\"])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def generate(model, idx, max_new_tokens, context_size, temperature, top_k=None):\n",
        "    idx = idx.to(device)\n",
        "    # For-loop is the same as before: Get logits, and only focus on last time step\n",
        "    for _ in range(max_new_tokens):\n",
        "        idx_cond = idx[:, -context_size:]\n",
        "        with torch.no_grad():\n",
        "            logits = model(idx_cond)\n",
        "        logits = logits[:, -1, :]\n",
        "\n",
        "        # New: Filter logits with top_k sampling\n",
        "        if top_k is not None:\n",
        "            # Keep only top_k values\n",
        "            top_logits, _ = torch.topk(logits, top_k)\n",
        "            min_val = top_logits[:, -1]\n",
        "            logits = torch.where(logits < min_val, torch.tensor(float('-inf')).to(logits.device), logits)\n",
        "\n",
        "        # New: Apply temperature scaling\n",
        "        if temperature > 0.0:\n",
        "            logits = logits / temperature\n",
        "\n",
        "            # Apply softmax to get probabilities\n",
        "            probs = torch.softmax(logits, dim=-1)  # (batch_size, context_len)\n",
        "\n",
        "            # Sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)  # (batch_size, 1)\n",
        "\n",
        "        # Otherwise same as before: get idx of the vocab entry with the highest logits value\n",
        "        else:\n",
        "            idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch_size, 1)\n",
        "\n",
        "        # Same as before: append sampled index to the running sequence\n",
        "        idx = torch.cat((idx, idx_next), dim=1)  # (batch_size, num_tokens+1)\n",
        "\n",
        "    return idx\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "model_names = {\n",
        "    \"gpt2-small\": \"openai-community/gpt2\",         # 124M\n",
        "    \"gpt2-medium\": \"openai-community/gpt2-medium\",  # 355M\n",
        "    \"gpt2-large\": \"openai-community/gpt2-large\",   # 774M\n",
        "    \"gpt2-xl\": \"openai-community/gpt2-xl\"          # 1558M\n",
        "}\n",
        "\n",
        "CHOOSE_MODEL = \"gpt2-small\"\n",
        "\n",
        "gpt_hf = GPT2Model.from_pretrained(\n",
        "    model_names[CHOOSE_MODEL], cache_dir=\"checkpoints\")\n",
        "gpt_hf.eval()\n",
        "\n",
        "\n",
        "BASE_CONFIG = {\n",
        "    \"vocab_size\": 50257,  # Vocabulary size\n",
        "    \"ctx_len\": 1024,      # Context length\n",
        "    \"drop_rate\": 0.1,     # Dropout rate\n",
        "    \"qkv_bias\": True      # Query-key-value bias\n",
        "}\n",
        "\n",
        "model_configs = {\n",
        "    \"gpt2-small\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
        "    \"gpt2-medium\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
        "    \"gpt2-large\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
        "    \"gpt2-xl\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
        "}\n",
        "\n",
        "text_test = \"1 c. firmly packed brown sugar 1/2 c. evaporated milk 1/2 tsp. vanilla 1/2 c. broken nuts (pecans) 2 Tbsp. butter or margarine 3 1/2 c. bite size shredded rice biscuits\\nInstruction: \"\n",
        "\n",
        "BASE_CONFIG.update(model_configs[CHOOSE_MODEL])\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = GPTModel(BASE_CONFIG)\n",
        "load_weights(model, gpt_hf)\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "\n",
        "# Frezzer layers\n",
        "\n",
        "# num_layer_frezzer = 12\n",
        "# for i in range(num_layer_frezzer):\n",
        "\n",
        "#     for param in model.trf_blocks[i].att.W_query.parameters():\n",
        "#         param.requires_grad = False\n",
        "\n",
        "#     for param in model.trf_blocks[i].att.W_key.parameters():\n",
        "#         param.requires_grad = False\n",
        "\n",
        "#     for param in model.trf_blocks[i].att.W_value.parameters():\n",
        "#         param.requires_grad = False\n",
        "\n",
        "#     for param in model.trf_blocks[i].att.out_proj.parameters():\n",
        "#         param.requires_grad = False\n",
        "\n",
        "#     for param in model.trf_blocks[i].ff.layers[0].parameters():\n",
        "#         param.requires_grad = False\n",
        "\n",
        "#     for param in model.trf_blocks[i].ff.layers[2].parameters():\n",
        "#         param.requires_grad = False\n",
        "\n",
        "#     for param in model.trf_blocks[i].norm1.parameters():\n",
        "#         param.requires_grad = False\n",
        "\n",
        "#     for param in model.trf_blocks[i].norm2.parameters():\n",
        "#         param.requires_grad = False\n",
        "\n",
        "# for name, param in model.named_parameters():\n",
        "#     print('{} {}'.format(name, param.requires_grad))\n",
        "\n",
        "\n",
        "\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "token_ids = generate(\n",
        "    model=model,\n",
        "    idx=text_to_token_ids(text_test, tokenizer),\n",
        "    max_new_tokens=256,\n",
        "    context_size=BASE_CONFIG[\"ctx_len\"],\n",
        "    top_k=1,\n",
        "    temperature=1.0\n",
        ")\n",
        "\n",
        "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "EqZOjBwqmIjr"
      },
      "outputs": [],
      "source": [
        "pred_gpt = \"\"\"Instruction:  1. Preheat oven to 350 degrees. In a large bowl, combine the flour, baking powder, baking soda, baking soda, baking soda, baking soda, baking soda, and 1/2 c. of the butter. In a bowl, combine the flour, baking soda, baking powder, baking soda, and 1/2 c. of the melted butter. In a large bowl, combine the flour, baking soda, baking powder, baking soda, and 1/2 c. of the melted butter. In a large bowl, combine the flour, baking powder, baking soda, baking soda, and 1/2 c. of the melted butter. In a large bowl, combine the flour, baking soda, baking soda, and 1/2 c. of the melted butter. In a large bowl, combine the melted butter, baking soda, and 1/2 c. of the melted butter. In a large bowl, combine the melted butter, baking soda, baking soda, baking powder, and 1/2 c. of the melted butter. In a large bowl, combine the melted butter, baking soda, and 1/2 c. of the melted butter. In a large bowl, combine the melted butter, baking soda, and 1/2 c.\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "TMDO-L68jQ6o"
      },
      "outputs": [],
      "source": [
        "pred_model = \"\"\"Intructions: Spread cookie sheet cake as directed on package and sprinkle over hot water. Stir until melted and smooth. Mix ingredients, nuts and raisins. Brush pan with the crumb mixture. Bake at 350° for 45 minutes. Makes 5 to 6 dozen.\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "rvMZubtJmeL9"
      },
      "outputs": [],
      "source": [
        "ref = 'Intructions: In a heavy 2-quart saucepan, mix brown sugar, nuts, evaporated milk and butter or margarine. Stir over medium heat until mixture bubbles all over top. Boil and stir 5 minutes more. Take off heat. Stir in vanilla and cereal; mix well. Using 2 teaspoons, drop and shape into 30 clusters on wax paper. Let stand until firm, about 30 minutes.'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xld-TXBXmOl8",
        "outputId": "3bc38919-9e7b-4231-c292-ed73d95e11fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.033707865168539325\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "\n",
        "hypothesis = pred_gpt.split(' ')\n",
        "reference = ref.split(' ')\n",
        "BLEUscore = nltk.translate.bleu_score.sentence_bleu([reference], hypothesis, weights = [1])\n",
        "print(BLEUscore)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4WC7TCn7mtbW",
        "outputId": "ed40a4e4-79cf-4de7-fd10-74b24297802d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.14974942948766023\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "\n",
        "hypothesis = pred_model.split(' ')\n",
        "reference = ref.split(' ')\n",
        "BLEUscore = nltk.translate.bleu_score.sentence_bleu([reference], hypothesis, weights = [1])\n",
        "print(BLEUscore)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dz_CWwZTmTJp",
        "outputId": "962a0484-6c32-4684-b8fa-83995a1fe979"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1 can green beans, wash and drain 1 can yellow beans, wash and drain 1 can kidney beans, wash and drain 1 c\n"
          ]
        }
      ],
      "source": [
        "text = \"1 can green beans, wash and drain 1 can yellow beans, wash and drain 1 can kidney beans, wash and drain 1 c. sugar 1/2 c. salad oil 2/3 c. vinegar 1/3 c. water Intructions: Mix together all of sugar, salad oil, vinegar and water until sugar is dissolved. Pour over beans and let stand overnight. salad sets the better it in the refrigerator overnight. This makes 6 to 6 hours before serving. closely. top of tomatoes, for it takes to 8 slices of cooking. love also use a salad of the last I go.?. for tomatoes and serve with waxed vegetables. to 6 than your take, th. much water as well in boiling water bath 5 minutes. will melt to 6. the rest. will 6 pints. 45 to 6 servings.tight container until ready to 6 servings. but very thin chunks. I use regularoeuvre. to 6 servings. courseigrams sodium per serving. dry. to 6 servings.., 1 vegetable reserved than sharp with waxoring. mg sodium. and gristle.. seasoning On a large servings. and serve immediately.ilk. will take peas and green pepper to 6 hours... cooking time; pour upside-fat layers. ham bone to 6 servings..ave until\"\n",
        "\n",
        "# Tìm vị trí của kí tự dừng\n",
        "stop_char_index = text.find('.')\n",
        "\n",
        "# Lấy phần văn bản từ đầu đến vị trí dừng\n",
        "substring = text[:stop_char_index]\n",
        "\n",
        "print(substring)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
