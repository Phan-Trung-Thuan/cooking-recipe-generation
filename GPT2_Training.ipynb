{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "Kb3OKNR3OeTh",
        "3swdAqPUKwr6",
        "G3oZN-kYOk42",
        "TY3-Cb13Os_v",
        "dESLtR9UO1Fe",
        "8DKDgO1_O6FA",
        "Fgto2MCoPM5u",
        "eVNFF60XLvz4"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1e94855d2c234009b1c37c1c5ed87b2f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3c45552dbd0c495fb8ca0c5d7cc1c5e5",
              "IPY_MODEL_666a911d9ccc4715997217451e7bb963",
              "IPY_MODEL_75b114820efb47158e882fc6060ee86b"
            ],
            "layout": "IPY_MODEL_12aeb9401df54da7b403770ccf4ca4e2"
          }
        },
        "3c45552dbd0c495fb8ca0c5d7cc1c5e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f46273a5fcb74c0fb07caec4dc31b2d1",
            "placeholder": "​",
            "style": "IPY_MODEL_913e05143b6c420983d15c4233997c12",
            "value": "config.json: 100%"
          }
        },
        "666a911d9ccc4715997217451e7bb963": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_012bad99c9c4474aaec498ef2f982f09",
            "max": 665,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_af39fe645e2d4adda0132e60a713c901",
            "value": 665
          }
        },
        "75b114820efb47158e882fc6060ee86b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_22f2cd400deb48e38e7705ae96e1848a",
            "placeholder": "​",
            "style": "IPY_MODEL_dffe61e06f254baf846c2b2f6fc83baa",
            "value": " 665/665 [00:00&lt;00:00, 4.70kB/s]"
          }
        },
        "12aeb9401df54da7b403770ccf4ca4e2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f46273a5fcb74c0fb07caec4dc31b2d1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "913e05143b6c420983d15c4233997c12": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "012bad99c9c4474aaec498ef2f982f09": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "af39fe645e2d4adda0132e60a713c901": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "22f2cd400deb48e38e7705ae96e1848a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dffe61e06f254baf846c2b2f6fc83baa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5b4d744b33b149088a22f959d0ac2d44": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7707be3f948646c6a8b8bcaf1b5412ae",
              "IPY_MODEL_efcc4b0242544793bd281f809086fe9b",
              "IPY_MODEL_0dab2247559049579b6cfe4e096ed877"
            ],
            "layout": "IPY_MODEL_3565e5c070b640f4a415912ff24d907c"
          }
        },
        "7707be3f948646c6a8b8bcaf1b5412ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ed33c52e140c489db3088f4656aa8fab",
            "placeholder": "​",
            "style": "IPY_MODEL_09d48ac690a842ba8e12350ebbcd46dc",
            "value": "model.safetensors: 100%"
          }
        },
        "efcc4b0242544793bd281f809086fe9b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aade5f28b49d45228ef270e9d84eb922",
            "max": 548105171,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_eee3a9a0dda043b9ac4e3d11663664a2",
            "value": 548105171
          }
        },
        "0dab2247559049579b6cfe4e096ed877": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_154b118488fe409f85c17e4d3c75a3cb",
            "placeholder": "​",
            "style": "IPY_MODEL_196e94014097459e8914f37d1c15b9b3",
            "value": " 548M/548M [00:08&lt;00:00, 64.6MB/s]"
          }
        },
        "3565e5c070b640f4a415912ff24d907c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ed33c52e140c489db3088f4656aa8fab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "09d48ac690a842ba8e12350ebbcd46dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "aade5f28b49d45228ef270e9d84eb922": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eee3a9a0dda043b9ac4e3d11663664a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "154b118488fe409f85c17e4d3c75a3cb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "196e94014097459e8914f37d1c15b9b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Install and import library"
      ],
      "metadata": {
        "id": "Kb3OKNR3OeTh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tiktoken"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aS92gamoKtoK",
        "outputId": "c8179c2c-1c9a-4b90-b258-d37050ea7ac6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.12.25)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.2.2)\n",
            "Installing collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "import tiktoken\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "from transformers import GPT2Model\n",
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "FVaz0rvKKiNS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocessing"
      ],
      "metadata": {
        "id": "3swdAqPUKwr6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dOr89mKTKdWY"
      },
      "outputs": [],
      "source": [
        "def preprocess(file_csv, tokenizer):\n",
        "    df = pd.read_csv(file_csv)\n",
        "    datasets = []\n",
        "\n",
        "    ingre = [' '.join(text for text in json.loads(df['ingredients'][i])) for i in range(len(df['ingredients']))]\n",
        "    direc = [' '.join(text for text in json.loads(df['directions'][i])) for i in range(len(df['directions']))]\n",
        "\n",
        "    texts = [ingre[i] + '\\nIntructions: ' + direc[i] for i in range(len(ingre))]\n",
        "\n",
        "    [datasets.append(torch.tensor(tokenizer.encode(text))) for text in texts]\n",
        "\n",
        "    return datasets\n",
        "\n",
        "\n",
        "def pad_and_truncate_sequences(sequences, max_length, padding_value=50256):\n",
        "    padded_sequences = []\n",
        "    for seq in sequences:\n",
        "        if len(seq) < max_length:\n",
        "            padding = [padding_value] * (max_length - len(seq))\n",
        "            padded_seq = torch.cat((seq, torch.tensor(padding)))\n",
        "            padded_sequences.append(padded_seq)\n",
        "        else:\n",
        "\n",
        "            padded_sequences.append(seq[:max_length])\n",
        "\n",
        "    padded_sequences = torch.stack(padded_sequences)\n",
        "\n",
        "    return padded_sequences\n",
        "\n",
        "def text_to_token_ids(text, tokenizer):\n",
        "    encoded = tokenizer.encode(text)\n",
        "    encoded_tensor = torch.tensor(encoded).unsqueeze(0)  # add batch dimension\n",
        "    return encoded_tensor\n",
        "\n",
        "def token_ids_to_text(token_ids, tokenizer):\n",
        "    flat = token_ids.squeeze(0)  # remove batch dimension\n",
        "    return tokenizer.decode(flat.tolist())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class RecipeDataset(Dataset):\n",
        "    def __init__(self, inp, out):\n",
        "        self.inp = inp\n",
        "        self.tar = out\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.inp)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.inp[idx], self.tar[idx]"
      ],
      "metadata": {
        "id": "D-JBO8qNLQ7J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Multi-head Attention"
      ],
      "metadata": {
        "id": "G3oZN-kYOk42"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_in, d_out, block_size, dropout, num_heads, qkv_bias=False):\n",
        "        super().__init__()\n",
        "        assert d_out % num_heads == 0, \"d_out must be divisible by n_heads\"\n",
        "\n",
        "        self.d_out = d_out\n",
        "        self.num_heads = num_heads\n",
        "        # Reduce the projection dim to match desired output dim\n",
        "        self.head_dim = d_out // num_heads\n",
        "\n",
        "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        # Linear layer to combine head outputs\n",
        "        self.out_proj = nn.Linear(d_out, d_out)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.register_buffer('mask', torch.triu(\n",
        "            torch.ones(block_size, block_size), diagonal=1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, num_tokens, d_in = x.shape\n",
        "\n",
        "        keys = self.W_key(x)  # Shape: (b, num_tokens, d_out)\n",
        "        queries = self.W_query(x)\n",
        "        values = self.W_value(x)\n",
        "\n",
        "        # We implicitly split the matrix by adding a `num_heads` dimension\n",
        "        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n",
        "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "\n",
        "        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n",
        "        keys = keys.transpose(1, 2)\n",
        "        queries = queries.transpose(1, 2)\n",
        "        values = values.transpose(1, 2)\n",
        "\n",
        "        # Compute scaled dot-product attention (aka self-attention) with a causal mask\n",
        "        # Dot product for each head\n",
        "        attn_scores = queries @ keys.transpose(2, 3)\n",
        "\n",
        "        # Original mask truncated to the number of tokens and converted to boolean\n",
        "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
        "\n",
        "        # Use the mask to fill attention scores\n",
        "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
        "\n",
        "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
        "        attn_weights = self.dropout(attn_weights)\n",
        "\n",
        "        # Shape: (b, num_tokens, num_heads, head_dim)\n",
        "        context_vec = (attn_weights @ values).transpose(1, 2)\n",
        "\n",
        "        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n",
        "        context_vec = context_vec.reshape(b, num_tokens, self.d_out)\n",
        "        context_vec = self.out_proj(context_vec)  # optional projection\n",
        "\n",
        "        return context_vec"
      ],
      "metadata": {
        "id": "yWTTYB45MxDe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Layer normalize + Feed forward (MLP)"
      ],
      "metadata": {
        "id": "TY3-Cb13Os_v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNorm(nn.Module):\n",
        "    def __init__(self, emb_dim):\n",
        "        super().__init__()\n",
        "        self.eps = 1e-5\n",
        "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
        "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = x.mean(dim=-1, keepdim=True)\n",
        "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
        "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
        "        return self.scale * norm_x + self.shift\n",
        "\n",
        "\n",
        "class GELU(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return 0.5 * x * (1 + torch.tanh(\n",
        "            torch.sqrt(torch.tensor(2.0 / torch.pi)) *\n",
        "            (x + 0.044715 * torch.pow(x, 3))\n",
        "        ))\n",
        "\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n",
        "            GELU(),\n",
        "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]),\n",
        "            nn.Dropout(cfg[\"drop_rate\"])\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)"
      ],
      "metadata": {
        "id": "wMZ8y_zENAXP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GPT2 Block"
      ],
      "metadata": {
        "id": "dESLtR9UO1Fe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.att = MultiHeadAttention(\n",
        "            d_in=cfg[\"emb_dim\"],\n",
        "            d_out=cfg[\"emb_dim\"],\n",
        "            block_size=cfg[\"seq_len\"],\n",
        "            num_heads=cfg[\"n_heads\"],\n",
        "            dropout=cfg[\"drop_rate\"],\n",
        "            qkv_bias=cfg[\"qkv_bias\"])\n",
        "        self.ff = FeedForward(cfg)\n",
        "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
        "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
        "        self.drop_resid = nn.Dropout(cfg[\"drop_rate\"])\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Shortcut connection for attention block\n",
        "        shortcut = x\n",
        "        x = self.norm1(x)\n",
        "        x = self.att(x)   # Shape [batch_size, num_tokens, emb_size]\n",
        "        x = self.drop_resid(x)\n",
        "        x = x + shortcut  # Add the original input back\n",
        "\n",
        "        # Shortcut connection for feed-forward block\n",
        "        shortcut = x\n",
        "        x = self.norm2(x)\n",
        "        x = self.ff(x)\n",
        "        x = self.drop_resid(x)\n",
        "        x = x + shortcut  # Add the original input back\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "wdOFmAcENR3l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GPT2 Model"
      ],
      "metadata": {
        "id": "8DKDgO1_O6FA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GPTModel(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
        "        self.pos_emb = nn.Embedding(cfg[\"seq_len\"], cfg[\"emb_dim\"])\n",
        "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
        "\n",
        "        self.trf_blocks = nn.Sequential(\n",
        "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
        "\n",
        "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
        "        self.out_head = nn.Linear(\n",
        "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False)\n",
        "\n",
        "    def forward(self, in_idx):\n",
        "        batch_size, seq_len = in_idx.shape\n",
        "        tok_embeds = self.tok_emb(in_idx)\n",
        "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
        "        x = tok_embeds + pos_embeds  # Shape [batch_size, num_tokens, emb_size]\n",
        "        x = self.drop_emb(x)\n",
        "        x = self.trf_blocks(x)\n",
        "        x = self.final_norm(x)\n",
        "        logits = self.out_head(x)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "Q5FSeEaRNJ2_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load GPT2 weights"
      ],
      "metadata": {
        "id": "Fgto2MCoPM5u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def assign_check(left, right):\n",
        "    if left.shape != right.shape:\n",
        "        raise ValueError(\n",
        "            f\"Shape mismatch. Left: {left.shape}, Right: {right.shape}\")\n",
        "    return torch.nn.Parameter(torch.tensor(right))\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def load_weights(gpt, gpt_hf):\n",
        "\n",
        "    d = gpt_hf.state_dict()\n",
        "\n",
        "    gpt.pos_emb.weight = assign_check(gpt.pos_emb.weight, d[\"wpe.weight\"])\n",
        "    gpt.tok_emb.weight = assign_check(gpt.tok_emb.weight, d[\"wte.weight\"])\n",
        "\n",
        "    for b in range(BASE_CONFIG[\"n_layers\"]):\n",
        "        q_w, k_w, v_w = np.split(d[f\"h.{b}.attn.c_attn.weight\"], 3, axis=-1)\n",
        "        gpt.trf_blocks[b].att.W_query.weight = assign_check(\n",
        "            gpt.trf_blocks[b].att.W_query.weight, q_w.T)\n",
        "        gpt.trf_blocks[b].att.W_key.weight = assign_check(\n",
        "            gpt.trf_blocks[b].att.W_key.weight, k_w.T)\n",
        "        gpt.trf_blocks[b].att.W_value.weight = assign_check(\n",
        "            gpt.trf_blocks[b].att.W_value.weight, v_w.T)\n",
        "\n",
        "        q_b, k_b, v_b = np.split(d[f\"h.{b}.attn.c_attn.bias\"], 3, axis=-1)\n",
        "        gpt.trf_blocks[b].att.W_query.bias = assign_check(\n",
        "            gpt.trf_blocks[b].att.W_query.bias, q_b)\n",
        "        gpt.trf_blocks[b].att.W_key.bias = assign_check(\n",
        "            gpt.trf_blocks[b].att.W_key.bias, k_b)\n",
        "        gpt.trf_blocks[b].att.W_value.bias = assign_check(\n",
        "            gpt.trf_blocks[b].att.W_value.bias, v_b)\n",
        "\n",
        "        gpt.trf_blocks[b].ff.layers[0].weight = assign_check(\n",
        "            gpt.trf_blocks[b].ff.layers[0].weight, d[f\"h.{b}.mlp.c_fc.weight\"].T)\n",
        "        gpt.trf_blocks[b].ff.layers[0].bias = assign_check(\n",
        "            gpt.trf_blocks[b].ff.layers[0].bias, d[f\"h.{b}.mlp.c_fc.bias\"])\n",
        "        gpt.trf_blocks[b].ff.layers[2].weight = assign_check(\n",
        "            gpt.trf_blocks[b].ff.layers[2].weight, d[f\"h.{b}.mlp.c_proj.weight\"].T)\n",
        "        gpt.trf_blocks[b].ff.layers[2].bias = assign_check(\n",
        "            gpt.trf_blocks[b].ff.layers[2].bias, d[f\"h.{b}.mlp.c_proj.bias\"])\n",
        "\n",
        "        gpt.trf_blocks[b].norm1.scale = assign_check(\n",
        "            gpt.trf_blocks[b].norm1.scale, d[f\"h.{b}.ln_1.weight\"])\n",
        "        gpt.trf_blocks[b].norm1.shift = assign_check(\n",
        "            gpt.trf_blocks[b].norm1.shift, d[f\"h.{b}.ln_1.bias\"])\n",
        "        gpt.trf_blocks[b].norm2.scale = assign_check(\n",
        "            gpt.trf_blocks[b].norm2.scale, d[f\"h.{b}.ln_2.weight\"])\n",
        "        gpt.trf_blocks[b].norm2.shift = assign_check(\n",
        "            gpt.trf_blocks[b].norm2.shift, d[f\"h.{b}.ln_2.bias\"])\n",
        "\n",
        "        gpt.trf_blocks[b].att.out_proj.weight = assign_check(\n",
        "            gpt.trf_blocks[b].att.out_proj.weight, d[f\"h.{b}.attn.c_proj.weight\"].T)\n",
        "        gpt.trf_blocks[b].att.out_proj.bias = assign_check(\n",
        "            gpt.trf_blocks[b].att.out_proj.bias, d[f\"h.{b}.attn.c_proj.bias\"])\n",
        "\n",
        "    gpt.final_norm.scale = assign_check(\n",
        "        gpt.final_norm.scale, d[f\"ln_f.weight\"])\n",
        "    gpt.final_norm.shift = assign_check(\n",
        "        gpt.final_norm.shift, d[f\"ln_f.bias\"])\n",
        "    gpt.out_head.weight = assign_check(\n",
        "        gpt.out_head.weight, d[\"wte.weight\"])"
      ],
      "metadata": {
        "id": "gFSJSkX7NigP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loss"
      ],
      "metadata": {
        "id": "eVNFF60XLvz4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_loss_batch(input_batch, target_batch, model, device):\n",
        "    mask = input_batch.view(-1) != 50256\n",
        "    mask = torch.tensor(mask)\n",
        "    mask = mask.to(device)\n",
        "\n",
        "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
        "\n",
        "    logits = model(input_batch)\n",
        "    logits = logits.view(-1, logits.size(-1))\n",
        "    loss = torch.nn.functional.cross_entropy(logits, target_batch.view(-1), reduction = 'none')\n",
        "\n",
        "    loss = loss*mask\n",
        "    loss = torch.sum(loss)/torch.sum(mask)\n",
        "\n",
        "    return loss\n",
        "\n",
        "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
        "    total_loss, batches_seen = 0., 0.\n",
        "    if num_batches is None:\n",
        "        num_batches = len(data_loader)\n",
        "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
        "        if i < num_batches:\n",
        "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
        "            total_loss += loss.item()\n",
        "            batches_seen += 1\n",
        "        else:\n",
        "            break\n",
        "    return total_loss / batches_seen"
      ],
      "metadata": {
        "id": "Cd2m2QMSLyOI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train model"
      ],
      "metadata": {
        "id": "A4Nx6ZJ5L7cI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs,\n",
        "                       eval_freq, eval_iter, start_context):\n",
        "    # Initialize lists to track losses and tokens seen\n",
        "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
        "    tokens_seen = 0\n",
        "    global_step = -1\n",
        "\n",
        "    # Main training loop\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()  # Set model to training mode\n",
        "\n",
        "        for input_batch, target_batch in train_loader:\n",
        "            optimizer.zero_grad()  # Reset loss gradients from previous epoch\n",
        "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
        "            loss.backward()  # Calculate loss gradients\n",
        "            optimizer.step()  # Update model weights using loss gradients\n",
        "            tokens_seen += input_batch.numel()\n",
        "            global_step += 1\n",
        "\n",
        "            # Optional evaluation step\n",
        "            if global_step % eval_freq == 0:\n",
        "                train_loss, val_loss = evaluate_model(\n",
        "                    model, train_loader, val_loader, device, eval_iter)\n",
        "                train_losses.append(train_loss)\n",
        "                val_losses.append(val_loss)\n",
        "                track_tokens_seen.append(tokens_seen)\n",
        "                print(f\"Ep {epoch+1} (Step {global_step:08d}): \"\n",
        "                      f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
        "\n",
        "        print('hee')\n",
        "        # Print a sample text after each epoch\n",
        "        generate_and_print_sample(\n",
        "            model, tokenizer, device, start_context\n",
        "        )\n",
        "\n",
        "        # torch.save(model.state_dict(), \"weights_model-gpt-medium-_best_ver2-ep{}.pth\".format(epoch+1))\n",
        "        # print('complete save model - epoch {}'.format(epoch + 1))\n",
        "    return train_losses, val_losses, track_tokens_seen\n",
        "\n",
        "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        train_loss = calc_loss_loader(\n",
        "            train_loader, model, device, num_batches=eval_iter)\n",
        "        val_loss = calc_loss_loader(\n",
        "            val_loader, model, device, num_batches=eval_iter)\n",
        "    model.train()\n",
        "    return train_loss, val_loss\n",
        "\n",
        "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
        "    # idx is (B, T) array of indices in the current context\n",
        "    for _ in range(max_new_tokens):\n",
        "\n",
        "        # Crop current context if it exceeds the supported context size\n",
        "        # E.g., if LLM supports only 5 tokens, and the context size is 10\n",
        "        # then only the last 5 tokens are used as context\n",
        "        idx_cond = idx[:, -context_size:]\n",
        "\n",
        "        # Get the predictions\n",
        "        with torch.no_grad():\n",
        "            logits = model(idx_cond)\n",
        "\n",
        "        # Focus only on the last time step\n",
        "        # (batch, n_token, vocab_size) becomes (batch, vocab_size)\n",
        "        logits = logits[:, -1, :]\n",
        "\n",
        "        # Get the idx of the vocab entry with the highest logits value\n",
        "        idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch, 1)\n",
        "\n",
        "        # Append sampled index to the running sequence\n",
        "        idx = torch.cat((idx, idx_next), dim=1)  # (batch, n_tokens+1)\n",
        "\n",
        "    return idx\n",
        "\n",
        "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
        "    model.eval()\n",
        "    context_size = model.pos_emb.weight.shape[0]\n",
        "    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
        "    with torch.no_grad():\n",
        "        token_ids = generate_text_simple(\n",
        "            model=model, idx=encoded,\n",
        "            max_new_tokens=60, context_size=context_size\n",
        "        )\n",
        "        decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
        "        print(decoded_text.replace(\"\\n\", \" \"))  # Compact print format\n",
        "    model.train()"
      ],
      "metadata": {
        "id": "7zYFi1eAL9jo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_csv = '/content/mini_RecipeNLG_2k5.csv'\n",
        "tokenizer = tiktoken.get_encoding('gpt2')\n",
        "max_length = 1025\n",
        "datasets = preprocess(file_csv, tokenizer)\n",
        "padded_and_truncated = pad_and_truncate_sequences(datasets, max_length)\n",
        "\n",
        "# print(len(datasets))\n",
        "# print(datasets[0].shape)\n",
        "# print(datasets[1].shape)\n",
        "# print(datasets[2].shape)\n",
        "# print(padded_and_truncated.shape)\n",
        "\n",
        "# Split datasets to trainData ValData\n",
        "train_ratio = 0.9\n",
        "indices = int(train_ratio*len(datasets))\n",
        "\n",
        "train_data, val_data = padded_and_truncated[:indices], padded_and_truncated[indices:]\n",
        "\n",
        "train_input, train_output = train_data[:, :-1], train_data[:, 1: ]\n",
        "val_input, val_output = val_data[:, :-1], val_data[:, 1: ]\n",
        "\n",
        "trainData = RecipeDataset(train_input, train_output)\n",
        "valData = RecipeDataset(val_input, val_output)\n",
        "\n",
        "trainloader = DataLoader(trainData, batch_size=4, shuffle=True, drop_last=False)\n",
        "valloader = DataLoader(valData, batch_size=4, shuffle=True, drop_last=False)"
      ],
      "metadata": {
        "id": "_qHHxxd_MPNo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_names = {\n",
        "    \"gpt2-small\": \"openai-community/gpt2\",         # 124M\n",
        "    \"gpt2-medium\": \"openai-community/gpt2-medium\",  # 355M\n",
        "    \"gpt2-large\": \"openai-community/gpt2-large\",   # 774M\n",
        "    \"gpt2-xl\": \"openai-community/gpt2-xl\"          # 1558M\n",
        "}\n",
        "\n",
        "CHOOSE_MODEL = \"gpt2-small\"\n",
        "\n",
        "gpt_hf = GPT2Model.from_pretrained(\n",
        "    model_names[CHOOSE_MODEL], cache_dir=\"checkpoints\")\n",
        "gpt_hf.eval()\n",
        "\n",
        "BASE_CONFIG = {\n",
        "    \"vocab_size\": 50257,  # Vocabulary size\n",
        "    \"seq_len\": 1024,      # Context length\n",
        "    \"drop_rate\": 0.2,     # Dropout rate\n",
        "    \"qkv_bias\": True      # Query-key-value bias\n",
        "}\n",
        "\n",
        "model_configs = {\n",
        "    \"gpt2-small\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
        "    \"gpt2-medium\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
        "    \"gpt2-large\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
        "    \"gpt2-xl\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
        "}\n",
        "\n",
        "BASE_CONFIG.update(model_configs[CHOOSE_MODEL])\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = GPTModel(BASE_CONFIG)\n",
        "load_weights(model, gpt_hf)"
      ],
      "metadata": {
        "id": "SYgnWGIWMtc1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 240,
          "referenced_widgets": [
            "1e94855d2c234009b1c37c1c5ed87b2f",
            "3c45552dbd0c495fb8ca0c5d7cc1c5e5",
            "666a911d9ccc4715997217451e7bb963",
            "75b114820efb47158e882fc6060ee86b",
            "12aeb9401df54da7b403770ccf4ca4e2",
            "f46273a5fcb74c0fb07caec4dc31b2d1",
            "913e05143b6c420983d15c4233997c12",
            "012bad99c9c4474aaec498ef2f982f09",
            "af39fe645e2d4adda0132e60a713c901",
            "22f2cd400deb48e38e7705ae96e1848a",
            "dffe61e06f254baf846c2b2f6fc83baa",
            "5b4d744b33b149088a22f959d0ac2d44",
            "7707be3f948646c6a8b8bcaf1b5412ae",
            "efcc4b0242544793bd281f809086fe9b",
            "0dab2247559049579b6cfe4e096ed877",
            "3565e5c070b640f4a415912ff24d907c",
            "ed33c52e140c489db3088f4656aa8fab",
            "09d48ac690a842ba8e12350ebbcd46dc",
            "aade5f28b49d45228ef270e9d84eb922",
            "eee3a9a0dda043b9ac4e3d11663664a2",
            "154b118488fe409f85c17e4d3c75a3cb",
            "196e94014097459e8914f37d1c15b9b3"
          ]
        },
        "outputId": "e8255713-b3f9-440a-ec65-38bfc4b7143f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1e94855d2c234009b1c37c1c5ed87b2f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5b4d744b33b149088a22f959d0ac2d44"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-f01d7bdc5872>:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  return torch.nn.Parameter(torch.tensor(right))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-4, weight_decay=0.1)\n",
        "\n",
        "num_epochs = 250\n",
        "\n",
        "text_test = \"1 c. firmly packed brown sugar 1/2 c. evaporated milk 1/2 tsp. vanilla 1/2 c. broken nuts (pecans) 2 Tbsp. butter or margarine 3 1/2 c. bite size shredded rice biscuits\\nInstruction: \"\n",
        "train_losses, val_losses, tokens_seen = train_model_simple(\n",
        "    model, trainloader, valloader, optimizer, device,\n",
        "    num_epochs=num_epochs, eval_freq=1, eval_iter=2,\n",
        "    start_context= text_test,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "8MfweCRLuDB4",
        "outputId": "e56ac46e-ca42-417c-9295-4392996ac2bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-421d0572aa36>:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  mask = torch.tensor(mask)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ep 1 (Step 00000000): Train loss 4.816, Val loss 4.487\n",
            "Ep 1 (Step 00000001): Train loss 4.886, Val loss 5.053\n",
            "Ep 1 (Step 00000002): Train loss 3.746, Val loss 3.698\n",
            "Ep 1 (Step 00000003): Train loss 3.907, Val loss 3.938\n",
            "Ep 1 (Step 00000004): Train loss 3.818, Val loss 3.548\n",
            "Ep 1 (Step 00000005): Train loss 3.693, Val loss 3.915\n",
            "Ep 1 (Step 00000006): Train loss 3.430, Val loss 3.675\n",
            "Ep 1 (Step 00000007): Train loss 3.356, Val loss 3.522\n",
            "Ep 1 (Step 00000008): Train loss 3.179, Val loss 3.500\n",
            "Ep 1 (Step 00000009): Train loss 3.293, Val loss 3.455\n",
            "Ep 1 (Step 00000010): Train loss 3.050, Val loss 3.628\n",
            "Ep 1 (Step 00000011): Train loss 3.506, Val loss 3.475\n",
            "Ep 1 (Step 00000012): Train loss 3.502, Val loss 3.118\n",
            "Ep 1 (Step 00000013): Train loss 3.284, Val loss 3.160\n",
            "Ep 1 (Step 00000014): Train loss 3.117, Val loss 3.179\n",
            "Ep 1 (Step 00000015): Train loss 3.327, Val loss 3.321\n",
            "Ep 1 (Step 00000016): Train loss 3.183, Val loss 3.118\n",
            "Ep 1 (Step 00000017): Train loss 2.967, Val loss 3.030\n",
            "Ep 1 (Step 00000018): Train loss 2.733, Val loss 2.940\n",
            "Ep 1 (Step 00000019): Train loss 3.238, Val loss 2.867\n",
            "Ep 1 (Step 00000020): Train loss 3.063, Val loss 3.193\n",
            "Ep 1 (Step 00000021): Train loss 2.799, Val loss 2.848\n",
            "Ep 1 (Step 00000022): Train loss 3.312, Val loss 2.988\n",
            "Ep 1 (Step 00000023): Train loss 2.898, Val loss 2.789\n",
            "Ep 1 (Step 00000024): Train loss 2.774, Val loss 2.816\n",
            "Ep 1 (Step 00000025): Train loss 2.700, Val loss 2.768\n",
            "Ep 1 (Step 00000026): Train loss 2.908, Val loss 2.700\n",
            "Ep 1 (Step 00000027): Train loss 3.032, Val loss 2.943\n",
            "Ep 1 (Step 00000028): Train loss 2.801, Val loss 2.564\n",
            "Ep 1 (Step 00000029): Train loss 2.691, Val loss 3.015\n",
            "Ep 1 (Step 00000030): Train loss 2.456, Val loss 2.865\n",
            "Ep 1 (Step 00000031): Train loss 2.463, Val loss 2.765\n",
            "Ep 1 (Step 00000032): Train loss 2.986, Val loss 2.606\n",
            "Ep 1 (Step 00000033): Train loss 2.940, Val loss 3.170\n",
            "Ep 1 (Step 00000034): Train loss 2.582, Val loss 2.908\n",
            "Ep 1 (Step 00000035): Train loss 2.559, Val loss 2.899\n",
            "Ep 1 (Step 00000036): Train loss 2.864, Val loss 2.745\n",
            "Ep 1 (Step 00000037): Train loss 2.440, Val loss 2.768\n",
            "Ep 1 (Step 00000038): Train loss 2.772, Val loss 2.548\n",
            "Ep 1 (Step 00000039): Train loss 2.616, Val loss 2.750\n",
            "Ep 1 (Step 00000040): Train loss 2.696, Val loss 2.478\n",
            "Ep 1 (Step 00000041): Train loss 2.911, Val loss 2.428\n",
            "Ep 1 (Step 00000042): Train loss 2.880, Val loss 2.789\n",
            "Ep 1 (Step 00000043): Train loss 2.884, Val loss 2.652\n",
            "Ep 1 (Step 00000044): Train loss 2.842, Val loss 3.004\n",
            "Ep 1 (Step 00000045): Train loss 2.573, Val loss 2.291\n",
            "Ep 1 (Step 00000046): Train loss 2.634, Val loss 2.569\n",
            "Ep 1 (Step 00000047): Train loss 2.787, Val loss 2.401\n",
            "Ep 1 (Step 00000048): Train loss 2.612, Val loss 3.234\n",
            "Ep 1 (Step 00000049): Train loss 2.639, Val loss 3.006\n",
            "Ep 1 (Step 00000050): Train loss 2.718, Val loss 2.526\n",
            "Ep 1 (Step 00000051): Train loss 2.280, Val loss 2.468\n",
            "Ep 1 (Step 00000052): Train loss 2.846, Val loss 2.703\n",
            "Ep 1 (Step 00000053): Train loss 2.605, Val loss 2.981\n",
            "Ep 1 (Step 00000054): Train loss 2.504, Val loss 2.392\n",
            "Ep 1 (Step 00000055): Train loss 2.683, Val loss 3.119\n",
            "Ep 1 (Step 00000056): Train loss 2.700, Val loss 2.692\n",
            "Ep 1 (Step 00000057): Train loss 2.606, Val loss 2.522\n",
            "Ep 1 (Step 00000058): Train loss 2.757, Val loss 3.000\n",
            "Ep 1 (Step 00000059): Train loss 2.881, Val loss 2.698\n",
            "Ep 1 (Step 00000060): Train loss 2.572, Val loss 2.807\n",
            "Ep 1 (Step 00000061): Train loss 2.480, Val loss 2.615\n",
            "Ep 1 (Step 00000062): Train loss 2.447, Val loss 2.485\n",
            "Ep 1 (Step 00000063): Train loss 2.657, Val loss 2.424\n",
            "Ep 1 (Step 00000064): Train loss 2.417, Val loss 2.688\n",
            "Ep 1 (Step 00000065): Train loss 2.418, Val loss 2.485\n",
            "Ep 1 (Step 00000066): Train loss 2.393, Val loss 2.554\n",
            "Ep 1 (Step 00000067): Train loss 2.667, Val loss 3.083\n",
            "Ep 1 (Step 00000068): Train loss 2.270, Val loss 2.615\n",
            "Ep 1 (Step 00000069): Train loss 2.580, Val loss 2.486\n",
            "Ep 1 (Step 00000070): Train loss 2.365, Val loss 2.514\n",
            "Ep 1 (Step 00000071): Train loss 2.614, Val loss 2.611\n",
            "Ep 1 (Step 00000072): Train loss 2.444, Val loss 2.406\n",
            "Ep 1 (Step 00000073): Train loss 2.371, Val loss 2.305\n",
            "Ep 1 (Step 00000074): Train loss 2.448, Val loss 2.468\n",
            "Ep 1 (Step 00000075): Train loss 2.686, Val loss 2.775\n",
            "Ep 1 (Step 00000076): Train loss 2.596, Val loss 2.930\n",
            "Ep 1 (Step 00000077): Train loss 2.472, Val loss 2.500\n",
            "Ep 1 (Step 00000078): Train loss 2.415, Val loss 2.485\n",
            "Ep 1 (Step 00000079): Train loss 2.529, Val loss 2.541\n",
            "Ep 1 (Step 00000080): Train loss 2.529, Val loss 2.416\n",
            "Ep 1 (Step 00000081): Train loss 2.710, Val loss 2.775\n",
            "Ep 1 (Step 00000082): Train loss 2.655, Val loss 2.419\n",
            "Ep 1 (Step 00000083): Train loss 2.494, Val loss 2.286\n",
            "Ep 1 (Step 00000084): Train loss 2.946, Val loss 2.377\n",
            "Ep 1 (Step 00000085): Train loss 2.739, Val loss 2.801\n",
            "Ep 1 (Step 00000086): Train loss 2.477, Val loss 2.733\n",
            "Ep 1 (Step 00000087): Train loss 2.483, Val loss 2.340\n",
            "Ep 1 (Step 00000088): Train loss 2.475, Val loss 2.520\n",
            "Ep 1 (Step 00000089): Train loss 2.612, Val loss 2.751\n",
            "Ep 1 (Step 00000090): Train loss 2.649, Val loss 2.679\n",
            "Ep 1 (Step 00000091): Train loss 2.245, Val loss 2.507\n",
            "Ep 1 (Step 00000092): Train loss 2.572, Val loss 2.660\n",
            "Ep 1 (Step 00000093): Train loss 2.722, Val loss 2.608\n",
            "Ep 1 (Step 00000094): Train loss 2.413, Val loss 2.520\n",
            "Ep 1 (Step 00000095): Train loss 2.578, Val loss 2.332\n",
            "Ep 1 (Step 00000096): Train loss 2.188, Val loss 2.360\n",
            "Ep 1 (Step 00000097): Train loss 2.653, Val loss 2.581\n",
            "Ep 1 (Step 00000098): Train loss 2.428, Val loss 2.280\n",
            "Ep 1 (Step 00000099): Train loss 2.531, Val loss 2.121\n",
            "Ep 1 (Step 00000100): Train loss 2.431, Val loss 2.584\n",
            "Ep 1 (Step 00000101): Train loss 2.413, Val loss 2.377\n",
            "Ep 1 (Step 00000102): Train loss 2.671, Val loss 2.680\n",
            "Ep 1 (Step 00000103): Train loss 2.399, Val loss 2.668\n",
            "Ep 1 (Step 00000104): Train loss 2.748, Val loss 2.680\n",
            "Ep 1 (Step 00000105): Train loss 2.553, Val loss 2.698\n",
            "Ep 1 (Step 00000106): Train loss 2.230, Val loss 2.535\n",
            "Ep 1 (Step 00000107): Train loss 2.296, Val loss 2.210\n",
            "Ep 1 (Step 00000108): Train loss 2.507, Val loss 2.792\n",
            "Ep 1 (Step 00000109): Train loss 2.572, Val loss 2.296\n",
            "Ep 1 (Step 00000110): Train loss 2.437, Val loss 2.504\n",
            "Ep 1 (Step 00000111): Train loss 2.568, Val loss 2.397\n",
            "Ep 1 (Step 00000112): Train loss 2.477, Val loss 2.526\n",
            "Ep 1 (Step 00000113): Train loss 2.389, Val loss 2.423\n",
            "Ep 1 (Step 00000114): Train loss 2.101, Val loss 2.670\n",
            "Ep 1 (Step 00000115): Train loss 2.622, Val loss 2.995\n",
            "Ep 1 (Step 00000116): Train loss 2.264, Val loss 2.600\n",
            "Ep 1 (Step 00000117): Train loss 2.517, Val loss 2.397\n",
            "Ep 1 (Step 00000118): Train loss 2.657, Val loss 2.977\n",
            "Ep 1 (Step 00000119): Train loss 2.228, Val loss 2.351\n",
            "Ep 1 (Step 00000120): Train loss 2.564, Val loss 2.350\n",
            "Ep 1 (Step 00000121): Train loss 2.286, Val loss 2.184\n",
            "Ep 1 (Step 00000122): Train loss 2.217, Val loss 2.400\n",
            "Ep 1 (Step 00000123): Train loss 2.580, Val loss 2.589\n",
            "Ep 1 (Step 00000124): Train loss 2.381, Val loss 2.656\n",
            "Ep 1 (Step 00000125): Train loss 2.436, Val loss 2.679\n",
            "Ep 1 (Step 00000126): Train loss 2.286, Val loss 2.230\n",
            "Ep 1 (Step 00000127): Train loss 2.459, Val loss 2.355\n",
            "Ep 1 (Step 00000128): Train loss 2.682, Val loss 2.274\n",
            "Ep 1 (Step 00000129): Train loss 2.468, Val loss 2.682\n",
            "Ep 1 (Step 00000130): Train loss 2.344, Val loss 2.629\n",
            "Ep 1 (Step 00000131): Train loss 2.712, Val loss 2.707\n",
            "Ep 1 (Step 00000132): Train loss 2.504, Val loss 2.336\n",
            "Ep 1 (Step 00000133): Train loss 2.433, Val loss 2.537\n",
            "Ep 1 (Step 00000134): Train loss 2.586, Val loss 2.578\n",
            "Ep 1 (Step 00000135): Train loss 2.491, Val loss 2.312\n",
            "Ep 1 (Step 00000136): Train loss 2.367, Val loss 2.556\n",
            "Ep 1 (Step 00000137): Train loss 2.239, Val loss 2.467\n",
            "Ep 1 (Step 00000138): Train loss 2.259, Val loss 2.577\n",
            "Ep 1 (Step 00000139): Train loss 2.598, Val loss 2.723\n",
            "Ep 1 (Step 00000140): Train loss 2.470, Val loss 2.192\n",
            "Ep 1 (Step 00000141): Train loss 2.401, Val loss 2.316\n",
            "Ep 1 (Step 00000142): Train loss 2.292, Val loss 2.451\n",
            "Ep 1 (Step 00000143): Train loss 2.355, Val loss 2.103\n",
            "Ep 1 (Step 00000144): Train loss 2.222, Val loss 2.355\n",
            "Ep 1 (Step 00000145): Train loss 2.573, Val loss 2.222\n",
            "Ep 1 (Step 00000146): Train loss 2.315, Val loss 2.357\n",
            "Ep 1 (Step 00000147): Train loss 2.344, Val loss 2.330\n",
            "Ep 1 (Step 00000148): Train loss 2.282, Val loss 2.688\n",
            "Ep 1 (Step 00000149): Train loss 2.313, Val loss 2.511\n",
            "Ep 1 (Step 00000150): Train loss 2.219, Val loss 2.342\n",
            "Ep 1 (Step 00000151): Train loss 2.444, Val loss 2.359\n",
            "Ep 1 (Step 00000152): Train loss 2.497, Val loss 2.385\n",
            "Ep 1 (Step 00000153): Train loss 2.320, Val loss 2.310\n",
            "Ep 1 (Step 00000154): Train loss 2.077, Val loss 2.543\n",
            "Ep 1 (Step 00000155): Train loss 2.367, Val loss 2.456\n",
            "Ep 1 (Step 00000156): Train loss 2.331, Val loss 2.528\n",
            "Ep 1 (Step 00000157): Train loss 2.375, Val loss 2.450\n",
            "Ep 1 (Step 00000158): Train loss 2.124, Val loss 2.534\n",
            "Ep 1 (Step 00000159): Train loss 2.364, Val loss 2.277\n",
            "Ep 1 (Step 00000160): Train loss 2.149, Val loss 2.271\n",
            "Ep 1 (Step 00000161): Train loss 2.435, Val loss 2.380\n",
            "Ep 1 (Step 00000162): Train loss 2.254, Val loss 2.370\n",
            "Ep 1 (Step 00000163): Train loss 2.251, Val loss 2.486\n",
            "Ep 1 (Step 00000164): Train loss 2.528, Val loss 2.394\n",
            "Ep 1 (Step 00000165): Train loss 2.354, Val loss 2.278\n",
            "Ep 1 (Step 00000166): Train loss 2.450, Val loss 2.293\n",
            "Ep 1 (Step 00000167): Train loss 2.403, Val loss 2.320\n",
            "Ep 1 (Step 00000168): Train loss 2.157, Val loss 2.528\n",
            "Ep 1 (Step 00000169): Train loss 2.242, Val loss 2.651\n",
            "Ep 1 (Step 00000170): Train loss 2.397, Val loss 2.420\n",
            "Ep 1 (Step 00000171): Train loss 2.353, Val loss 2.395\n",
            "Ep 1 (Step 00000172): Train loss 2.260, Val loss 2.624\n",
            "Ep 1 (Step 00000173): Train loss 2.322, Val loss 2.348\n",
            "Ep 1 (Step 00000174): Train loss 2.299, Val loss 2.437\n",
            "Ep 1 (Step 00000175): Train loss 2.473, Val loss 2.416\n",
            "Ep 1 (Step 00000176): Train loss 2.514, Val loss 2.673\n",
            "Ep 1 (Step 00000177): Train loss 2.544, Val loss 2.297\n",
            "Ep 1 (Step 00000178): Train loss 2.155, Val loss 2.284\n",
            "Ep 1 (Step 00000179): Train loss 2.393, Val loss 2.508\n",
            "Ep 1 (Step 00000180): Train loss 2.566, Val loss 2.049\n",
            "Ep 1 (Step 00000181): Train loss 2.521, Val loss 2.461\n",
            "Ep 1 (Step 00000182): Train loss 2.067, Val loss 2.378\n",
            "Ep 1 (Step 00000183): Train loss 2.147, Val loss 2.387\n",
            "Ep 1 (Step 00000184): Train loss 2.224, Val loss 2.594\n",
            "Ep 1 (Step 00000185): Train loss 2.201, Val loss 2.393\n",
            "Ep 1 (Step 00000186): Train loss 2.526, Val loss 2.431\n",
            "Ep 1 (Step 00000187): Train loss 2.337, Val loss 2.733\n",
            "Ep 1 (Step 00000188): Train loss 2.406, Val loss 2.612\n",
            "Ep 1 (Step 00000189): Train loss 2.418, Val loss 2.397\n",
            "Ep 1 (Step 00000190): Train loss 2.032, Val loss 2.348\n",
            "Ep 1 (Step 00000191): Train loss 2.030, Val loss 2.315\n",
            "Ep 1 (Step 00000192): Train loss 2.312, Val loss 2.321\n",
            "Ep 1 (Step 00000193): Train loss 2.331, Val loss 2.405\n",
            "Ep 1 (Step 00000194): Train loss 2.220, Val loss 2.123\n",
            "Ep 1 (Step 00000195): Train loss 2.339, Val loss 2.214\n",
            "Ep 1 (Step 00000196): Train loss 2.703, Val loss 2.424\n",
            "Ep 1 (Step 00000197): Train loss 2.150, Val loss 2.571\n",
            "Ep 1 (Step 00000198): Train loss 2.280, Val loss 2.176\n",
            "Ep 1 (Step 00000199): Train loss 2.443, Val loss 2.615\n",
            "Ep 1 (Step 00000200): Train loss 2.100, Val loss 2.465\n",
            "Ep 1 (Step 00000201): Train loss 2.212, Val loss 2.688\n",
            "Ep 1 (Step 00000202): Train loss 2.416, Val loss 2.741\n",
            "Ep 1 (Step 00000203): Train loss 2.108, Val loss 2.426\n",
            "Ep 1 (Step 00000204): Train loss 2.219, Val loss 2.244\n",
            "Ep 1 (Step 00000205): Train loss 2.208, Val loss 2.556\n",
            "Ep 1 (Step 00000206): Train loss 2.262, Val loss 2.177\n",
            "Ep 1 (Step 00000207): Train loss 2.350, Val loss 2.645\n",
            "Ep 1 (Step 00000208): Train loss 2.357, Val loss 2.451\n",
            "Ep 1 (Step 00000209): Train loss 2.048, Val loss 2.089\n",
            "Ep 1 (Step 00000210): Train loss 2.127, Val loss 2.368\n",
            "Ep 1 (Step 00000211): Train loss 2.296, Val loss 2.392\n",
            "Ep 1 (Step 00000212): Train loss 2.206, Val loss 2.425\n",
            "Ep 1 (Step 00000213): Train loss 2.134, Val loss 2.429\n",
            "Ep 1 (Step 00000214): Train loss 2.504, Val loss 2.358\n",
            "Ep 1 (Step 00000215): Train loss 2.303, Val loss 2.059\n",
            "Ep 1 (Step 00000216): Train loss 2.314, Val loss 2.291\n",
            "Ep 1 (Step 00000217): Train loss 2.650, Val loss 2.441\n",
            "Ep 1 (Step 00000218): Train loss 2.353, Val loss 2.042\n",
            "Ep 1 (Step 00000219): Train loss 2.149, Val loss 2.586\n",
            "Ep 1 (Step 00000220): Train loss 2.477, Val loss 2.115\n",
            "Ep 1 (Step 00000221): Train loss 2.175, Val loss 2.515\n",
            "Ep 1 (Step 00000222): Train loss 2.133, Val loss 2.641\n",
            "Ep 1 (Step 00000223): Train loss 2.169, Val loss 2.353\n",
            "Ep 1 (Step 00000224): Train loss 2.628, Val loss 2.401\n",
            "Ep 1 (Step 00000225): Train loss 2.214, Val loss 2.343\n",
            "Ep 1 (Step 00000226): Train loss 2.506, Val loss 2.507\n",
            "Ep 1 (Step 00000227): Train loss 2.484, Val loss 2.156\n",
            "Ep 1 (Step 00000228): Train loss 2.044, Val loss 2.548\n",
            "Ep 1 (Step 00000229): Train loss 2.354, Val loss 2.304\n",
            "Ep 1 (Step 00000230): Train loss 1.916, Val loss 2.198\n",
            "Ep 1 (Step 00000231): Train loss 2.027, Val loss 2.141\n",
            "Ep 1 (Step 00000232): Train loss 2.184, Val loss 2.709\n",
            "Ep 1 (Step 00000233): Train loss 2.348, Val loss 2.137\n",
            "Ep 1 (Step 00000234): Train loss 2.311, Val loss 2.126\n",
            "Ep 1 (Step 00000235): Train loss 2.427, Val loss 2.051\n",
            "Ep 1 (Step 00000236): Train loss 2.552, Val loss 2.802\n",
            "Ep 1 (Step 00000237): Train loss 2.343, Val loss 2.566\n",
            "Ep 1 (Step 00000238): Train loss 2.106, Val loss 2.158\n",
            "Ep 1 (Step 00000239): Train loss 2.324, Val loss 2.108\n",
            "Ep 1 (Step 00000240): Train loss 2.060, Val loss 2.465\n",
            "Ep 1 (Step 00000241): Train loss 2.242, Val loss 2.156\n",
            "Ep 1 (Step 00000242): Train loss 2.074, Val loss 2.853\n",
            "Ep 1 (Step 00000243): Train loss 2.323, Val loss 2.598\n",
            "Ep 1 (Step 00000244): Train loss 2.064, Val loss 2.950\n",
            "Ep 1 (Step 00000245): Train loss 2.085, Val loss 2.586\n",
            "Ep 1 (Step 00000246): Train loss 2.329, Val loss 2.496\n",
            "Ep 1 (Step 00000247): Train loss 2.090, Val loss 2.617\n",
            "Ep 1 (Step 00000248): Train loss 2.410, Val loss 2.521\n",
            "Ep 1 (Step 00000249): Train loss 2.139, Val loss 2.626\n",
            "Ep 1 (Step 00000250): Train loss 2.301, Val loss 2.708\n",
            "Ep 1 (Step 00000251): Train loss 2.355, Val loss 2.639\n",
            "Ep 1 (Step 00000252): Train loss 2.117, Val loss 2.296\n",
            "Ep 1 (Step 00000253): Train loss 2.294, Val loss 2.289\n",
            "Ep 1 (Step 00000254): Train loss 2.267, Val loss 2.053\n",
            "Ep 1 (Step 00000255): Train loss 1.858, Val loss 2.278\n",
            "Ep 1 (Step 00000256): Train loss 1.946, Val loss 2.020\n",
            "Ep 1 (Step 00000257): Train loss 2.064, Val loss 2.447\n",
            "Ep 1 (Step 00000258): Train loss 1.938, Val loss 2.461\n",
            "Ep 1 (Step 00000259): Train loss 2.093, Val loss 2.263\n",
            "Ep 1 (Step 00000260): Train loss 1.765, Val loss 2.435\n",
            "Ep 1 (Step 00000261): Train loss 2.189, Val loss 2.216\n",
            "Ep 1 (Step 00000262): Train loss 2.383, Val loss 2.300\n",
            "Ep 1 (Step 00000263): Train loss 2.294, Val loss 2.199\n",
            "Ep 1 (Step 00000264): Train loss 1.979, Val loss 2.547\n",
            "Ep 1 (Step 00000265): Train loss 2.316, Val loss 2.267\n",
            "Ep 1 (Step 00000266): Train loss 2.116, Val loss 2.286\n",
            "Ep 1 (Step 00000267): Train loss 2.316, Val loss 2.451\n",
            "Ep 1 (Step 00000268): Train loss 1.921, Val loss 2.290\n",
            "Ep 1 (Step 00000269): Train loss 2.137, Val loss 2.621\n",
            "Ep 1 (Step 00000270): Train loss 2.535, Val loss 2.311\n",
            "Ep 1 (Step 00000271): Train loss 2.224, Val loss 2.647\n",
            "Ep 1 (Step 00000272): Train loss 2.362, Val loss 2.276\n",
            "Ep 1 (Step 00000273): Train loss 2.310, Val loss 2.502\n",
            "Ep 1 (Step 00000274): Train loss 2.187, Val loss 2.706\n",
            "Ep 1 (Step 00000275): Train loss 2.266, Val loss 2.314\n",
            "Ep 1 (Step 00000276): Train loss 2.311, Val loss 2.231\n",
            "Ep 1 (Step 00000277): Train loss 2.191, Val loss 2.212\n",
            "Ep 1 (Step 00000278): Train loss 2.410, Val loss 2.709\n",
            "Ep 1 (Step 00000279): Train loss 1.986, Val loss 2.226\n",
            "Ep 1 (Step 00000280): Train loss 2.220, Val loss 2.388\n",
            "Ep 1 (Step 00000281): Train loss 1.855, Val loss 2.150\n",
            "Ep 1 (Step 00000282): Train loss 2.299, Val loss 2.181\n",
            "Ep 1 (Step 00000283): Train loss 2.083, Val loss 2.297\n",
            "Ep 1 (Step 00000284): Train loss 2.209, Val loss 2.188\n",
            "Ep 1 (Step 00000285): Train loss 2.372, Val loss 2.709\n",
            "Ep 1 (Step 00000286): Train loss 2.318, Val loss 1.986\n",
            "Ep 1 (Step 00000287): Train loss 2.017, Val loss 2.172\n",
            "Ep 1 (Step 00000288): Train loss 2.028, Val loss 2.337\n",
            "Ep 1 (Step 00000289): Train loss 2.267, Val loss 2.215\n",
            "Ep 1 (Step 00000290): Train loss 1.995, Val loss 2.225\n",
            "Ep 1 (Step 00000291): Train loss 2.199, Val loss 2.249\n",
            "Ep 1 (Step 00000292): Train loss 2.019, Val loss 2.584\n",
            "Ep 1 (Step 00000293): Train loss 2.261, Val loss 2.344\n",
            "Ep 1 (Step 00000294): Train loss 2.261, Val loss 2.204\n",
            "Ep 1 (Step 00000295): Train loss 1.896, Val loss 2.269\n",
            "Ep 1 (Step 00000296): Train loss 2.474, Val loss 2.307\n",
            "Ep 1 (Step 00000297): Train loss 2.015, Val loss 2.561\n",
            "Ep 1 (Step 00000298): Train loss 2.154, Val loss 2.572\n",
            "Ep 1 (Step 00000299): Train loss 2.098, Val loss 2.217\n",
            "Ep 1 (Step 00000300): Train loss 2.119, Val loss 2.576\n",
            "Ep 1 (Step 00000301): Train loss 2.413, Val loss 2.221\n",
            "Ep 1 (Step 00000302): Train loss 2.045, Val loss 2.262\n",
            "Ep 1 (Step 00000303): Train loss 2.012, Val loss 2.165\n",
            "Ep 1 (Step 00000304): Train loss 2.208, Val loss 2.641\n",
            "Ep 1 (Step 00000305): Train loss 1.933, Val loss 2.532\n",
            "Ep 1 (Step 00000306): Train loss 1.965, Val loss 2.348\n",
            "Ep 1 (Step 00000307): Train loss 2.049, Val loss 2.431\n",
            "Ep 1 (Step 00000308): Train loss 1.943, Val loss 2.619\n",
            "Ep 1 (Step 00000309): Train loss 2.004, Val loss 2.291\n",
            "Ep 1 (Step 00000310): Train loss 2.098, Val loss 2.411\n",
            "Ep 1 (Step 00000311): Train loss 2.084, Val loss 1.988\n",
            "Ep 1 (Step 00000312): Train loss 2.054, Val loss 2.264\n",
            "Ep 1 (Step 00000313): Train loss 1.994, Val loss 2.129\n",
            "Ep 1 (Step 00000314): Train loss 1.985, Val loss 2.402\n",
            "Ep 1 (Step 00000315): Train loss 1.841, Val loss 2.519\n",
            "Ep 1 (Step 00000316): Train loss 2.250, Val loss 2.321\n",
            "Ep 1 (Step 00000317): Train loss 2.141, Val loss 2.229\n",
            "Ep 1 (Step 00000318): Train loss 2.225, Val loss 2.438\n",
            "Ep 1 (Step 00000319): Train loss 2.408, Val loss 2.194\n",
            "Ep 1 (Step 00000320): Train loss 2.201, Val loss 2.090\n",
            "Ep 1 (Step 00000321): Train loss 2.363, Val loss 2.303\n",
            "Ep 1 (Step 00000322): Train loss 2.007, Val loss 2.180\n",
            "Ep 1 (Step 00000323): Train loss 2.480, Val loss 2.624\n",
            "Ep 1 (Step 00000324): Train loss 1.995, Val loss 2.188\n",
            "Ep 1 (Step 00000325): Train loss 2.375, Val loss 2.325\n",
            "Ep 1 (Step 00000326): Train loss 2.336, Val loss 2.355\n",
            "Ep 1 (Step 00000327): Train loss 2.045, Val loss 2.379\n",
            "Ep 1 (Step 00000328): Train loss 2.091, Val loss 2.195\n",
            "Ep 1 (Step 00000329): Train loss 2.057, Val loss 2.374\n",
            "Ep 1 (Step 00000330): Train loss 2.117, Val loss 2.169\n",
            "Ep 1 (Step 00000331): Train loss 2.057, Val loss 2.215\n",
            "Ep 1 (Step 00000332): Train loss 2.311, Val loss 2.292\n",
            "Ep 1 (Step 00000333): Train loss 2.402, Val loss 2.614\n",
            "Ep 1 (Step 00000334): Train loss 1.975, Val loss 2.606\n",
            "Ep 1 (Step 00000335): Train loss 2.195, Val loss 2.292\n",
            "Ep 1 (Step 00000336): Train loss 2.259, Val loss 2.098\n",
            "Ep 1 (Step 00000337): Train loss 2.383, Val loss 2.282\n",
            "Ep 1 (Step 00000338): Train loss 2.387, Val loss 2.227\n",
            "Ep 1 (Step 00000339): Train loss 2.061, Val loss 2.227\n",
            "Ep 1 (Step 00000340): Train loss 2.139, Val loss 2.084\n",
            "Ep 1 (Step 00000341): Train loss 2.175, Val loss 2.256\n",
            "Ep 1 (Step 00000342): Train loss 2.215, Val loss 2.769\n",
            "Ep 1 (Step 00000343): Train loss 2.279, Val loss 2.481\n",
            "Ep 1 (Step 00000344): Train loss 2.118, Val loss 2.124\n",
            "Ep 1 (Step 00000345): Train loss 2.057, Val loss 2.399\n",
            "Ep 1 (Step 00000346): Train loss 2.140, Val loss 2.315\n",
            "Ep 1 (Step 00000347): Train loss 2.283, Val loss 2.373\n",
            "Ep 1 (Step 00000348): Train loss 2.157, Val loss 2.567\n",
            "Ep 1 (Step 00000349): Train loss 1.994, Val loss 1.866\n",
            "Ep 1 (Step 00000350): Train loss 2.089, Val loss 2.401\n",
            "Ep 1 (Step 00000351): Train loss 1.977, Val loss 2.458\n",
            "Ep 1 (Step 00000352): Train loss 2.049, Val loss 2.614\n",
            "Ep 1 (Step 00000353): Train loss 2.375, Val loss 2.193\n",
            "Ep 1 (Step 00000354): Train loss 2.524, Val loss 2.287\n",
            "Ep 1 (Step 00000355): Train loss 2.085, Val loss 2.986\n",
            "Ep 1 (Step 00000356): Train loss 2.067, Val loss 2.423\n",
            "Ep 1 (Step 00000357): Train loss 2.129, Val loss 2.159\n",
            "Ep 1 (Step 00000358): Train loss 1.995, Val loss 2.272\n",
            "Ep 1 (Step 00000359): Train loss 2.196, Val loss 2.367\n",
            "Ep 1 (Step 00000360): Train loss 2.457, Val loss 2.375\n",
            "Ep 1 (Step 00000361): Train loss 2.182, Val loss 2.168\n",
            "Ep 1 (Step 00000362): Train loss 1.860, Val loss 2.143\n",
            "Ep 1 (Step 00000363): Train loss 1.972, Val loss 2.284\n",
            "Ep 1 (Step 00000364): Train loss 2.423, Val loss 2.214\n",
            "Ep 1 (Step 00000365): Train loss 2.029, Val loss 2.177\n",
            "Ep 1 (Step 00000366): Train loss 2.032, Val loss 1.932\n",
            "Ep 1 (Step 00000367): Train loss 2.105, Val loss 2.393\n",
            "Ep 1 (Step 00000368): Train loss 2.333, Val loss 2.392\n",
            "Ep 1 (Step 00000369): Train loss 2.158, Val loss 2.409\n",
            "Ep 1 (Step 00000370): Train loss 1.844, Val loss 2.073\n",
            "Ep 1 (Step 00000371): Train loss 2.211, Val loss 2.386\n",
            "Ep 1 (Step 00000372): Train loss 2.308, Val loss 2.501\n",
            "Ep 1 (Step 00000373): Train loss 2.457, Val loss 2.090\n",
            "Ep 1 (Step 00000374): Train loss 2.090, Val loss 2.120\n",
            "Ep 1 (Step 00000375): Train loss 2.221, Val loss 2.725\n",
            "Ep 1 (Step 00000376): Train loss 2.003, Val loss 2.483\n",
            "Ep 1 (Step 00000377): Train loss 1.877, Val loss 2.364\n",
            "Ep 1 (Step 00000378): Train loss 1.762, Val loss 2.779\n",
            "Ep 1 (Step 00000379): Train loss 1.830, Val loss 2.365\n",
            "Ep 1 (Step 00000380): Train loss 2.023, Val loss 2.329\n",
            "Ep 1 (Step 00000381): Train loss 2.145, Val loss 2.182\n",
            "Ep 1 (Step 00000382): Train loss 1.859, Val loss 2.131\n",
            "Ep 1 (Step 00000383): Train loss 1.939, Val loss 2.481\n",
            "Ep 1 (Step 00000384): Train loss 2.071, Val loss 2.277\n",
            "Ep 1 (Step 00000385): Train loss 1.946, Val loss 2.347\n",
            "Ep 1 (Step 00000386): Train loss 2.176, Val loss 2.198\n",
            "Ep 1 (Step 00000387): Train loss 2.393, Val loss 2.436\n",
            "Ep 1 (Step 00000388): Train loss 2.134, Val loss 2.601\n",
            "Ep 1 (Step 00000389): Train loss 1.906, Val loss 2.057\n",
            "Ep 1 (Step 00000390): Train loss 1.821, Val loss 2.605\n",
            "Ep 1 (Step 00000391): Train loss 2.136, Val loss 2.051\n",
            "Ep 1 (Step 00000392): Train loss 2.031, Val loss 2.732\n",
            "Ep 1 (Step 00000393): Train loss 2.255, Val loss 1.974\n",
            "Ep 1 (Step 00000394): Train loss 2.117, Val loss 2.209\n",
            "Ep 1 (Step 00000395): Train loss 2.264, Val loss 2.188\n",
            "Ep 1 (Step 00000396): Train loss 1.884, Val loss 2.109\n",
            "Ep 1 (Step 00000397): Train loss 2.155, Val loss 2.540\n",
            "Ep 1 (Step 00000398): Train loss 1.768, Val loss 2.204\n",
            "Ep 1 (Step 00000399): Train loss 2.176, Val loss 2.344\n",
            "Ep 1 (Step 00000400): Train loss 1.694, Val loss 2.346\n",
            "Ep 1 (Step 00000401): Train loss 1.822, Val loss 2.233\n",
            "Ep 1 (Step 00000402): Train loss 2.202, Val loss 2.205\n",
            "Ep 1 (Step 00000403): Train loss 1.830, Val loss 2.229\n",
            "Ep 1 (Step 00000404): Train loss 2.102, Val loss 2.248\n",
            "Ep 1 (Step 00000405): Train loss 1.803, Val loss 2.327\n",
            "Ep 1 (Step 00000406): Train loss 2.340, Val loss 2.044\n",
            "Ep 1 (Step 00000407): Train loss 2.349, Val loss 2.348\n",
            "Ep 1 (Step 00000408): Train loss 2.182, Val loss 2.159\n",
            "Ep 1 (Step 00000409): Train loss 2.197, Val loss 2.422\n",
            "Ep 1 (Step 00000410): Train loss 2.017, Val loss 2.451\n",
            "Ep 1 (Step 00000411): Train loss 1.806, Val loss 2.207\n",
            "Ep 1 (Step 00000412): Train loss 1.959, Val loss 2.380\n",
            "Ep 1 (Step 00000413): Train loss 2.335, Val loss 2.345\n",
            "Ep 1 (Step 00000414): Train loss 2.126, Val loss 2.605\n",
            "Ep 1 (Step 00000415): Train loss 1.992, Val loss 2.651\n",
            "Ep 1 (Step 00000416): Train loss 2.229, Val loss 2.261\n",
            "Ep 1 (Step 00000417): Train loss 2.181, Val loss 2.249\n",
            "Ep 1 (Step 00000418): Train loss 2.400, Val loss 2.224\n",
            "Ep 1 (Step 00000419): Train loss 1.900, Val loss 1.942\n",
            "Ep 1 (Step 00000420): Train loss 1.835, Val loss 2.343\n",
            "Ep 1 (Step 00000421): Train loss 2.056, Val loss 2.268\n",
            "Ep 1 (Step 00000422): Train loss 1.768, Val loss 1.986\n",
            "Ep 1 (Step 00000423): Train loss 2.066, Val loss 2.078\n",
            "Ep 1 (Step 00000424): Train loss 1.900, Val loss 2.107\n",
            "Ep 1 (Step 00000425): Train loss 2.188, Val loss 2.588\n",
            "Ep 1 (Step 00000426): Train loss 2.103, Val loss 2.363\n",
            "Ep 1 (Step 00000427): Train loss 2.062, Val loss 2.317\n",
            "Ep 1 (Step 00000428): Train loss 1.811, Val loss 2.491\n",
            "Ep 1 (Step 00000429): Train loss 2.197, Val loss 2.449\n",
            "Ep 1 (Step 00000430): Train loss 2.131, Val loss 2.338\n",
            "Ep 1 (Step 00000431): Train loss 2.074, Val loss 2.607\n",
            "Ep 1 (Step 00000432): Train loss 1.954, Val loss 2.111\n",
            "Ep 1 (Step 00000433): Train loss 2.147, Val loss 2.332\n",
            "Ep 1 (Step 00000434): Train loss 2.094, Val loss 2.378\n",
            "Ep 1 (Step 00000435): Train loss 1.946, Val loss 2.344\n",
            "Ep 1 (Step 00000436): Train loss 1.930, Val loss 2.237\n",
            "Ep 1 (Step 00000437): Train loss 2.129, Val loss 2.575\n",
            "Ep 1 (Step 00000438): Train loss 1.977, Val loss 2.260\n",
            "Ep 1 (Step 00000439): Train loss 2.149, Val loss 2.501\n",
            "Ep 1 (Step 00000440): Train loss 2.163, Val loss 2.436\n",
            "Ep 1 (Step 00000441): Train loss 1.948, Val loss 2.059\n",
            "Ep 1 (Step 00000442): Train loss 2.071, Val loss 2.262\n",
            "Ep 1 (Step 00000443): Train loss 2.292, Val loss 2.099\n",
            "Ep 1 (Step 00000444): Train loss 2.187, Val loss 2.560\n",
            "Ep 1 (Step 00000445): Train loss 1.936, Val loss 2.358\n",
            "Ep 1 (Step 00000446): Train loss 2.123, Val loss 2.041\n",
            "Ep 1 (Step 00000447): Train loss 2.049, Val loss 2.228\n",
            "Ep 1 (Step 00000448): Train loss 2.255, Val loss 2.308\n",
            "Ep 1 (Step 00000449): Train loss 2.058, Val loss 2.574\n",
            "Ep 1 (Step 00000450): Train loss 2.031, Val loss 1.851\n",
            "Ep 1 (Step 00000451): Train loss 2.072, Val loss 2.241\n",
            "Ep 1 (Step 00000452): Train loss 2.225, Val loss 2.515\n",
            "Ep 1 (Step 00000453): Train loss 2.016, Val loss 2.932\n",
            "Ep 1 (Step 00000454): Train loss 2.161, Val loss 2.263\n",
            "Ep 1 (Step 00000455): Train loss 2.075, Val loss 2.048\n",
            "Ep 1 (Step 00000456): Train loss 2.563, Val loss 1.978\n",
            "Ep 1 (Step 00000457): Train loss 1.956, Val loss 1.944\n",
            "Ep 1 (Step 00000458): Train loss 2.400, Val loss 2.094\n",
            "Ep 1 (Step 00000459): Train loss 1.810, Val loss 1.952\n",
            "Ep 1 (Step 00000460): Train loss 2.115, Val loss 2.304\n",
            "Ep 1 (Step 00000461): Train loss 2.403, Val loss 2.236\n",
            "Ep 1 (Step 00000462): Train loss 2.089, Val loss 2.490\n",
            "Ep 1 (Step 00000463): Train loss 2.144, Val loss 2.687\n",
            "Ep 1 (Step 00000464): Train loss 1.758, Val loss 2.335\n",
            "Ep 1 (Step 00000465): Train loss 2.018, Val loss 2.262\n",
            "Ep 1 (Step 00000466): Train loss 1.944, Val loss 2.422\n",
            "Ep 1 (Step 00000467): Train loss 2.074, Val loss 2.038\n",
            "Ep 1 (Step 00000468): Train loss 1.901, Val loss 2.760\n",
            "Ep 1 (Step 00000469): Train loss 1.995, Val loss 2.146\n",
            "Ep 1 (Step 00000470): Train loss 2.145, Val loss 2.058\n",
            "Ep 1 (Step 00000471): Train loss 1.898, Val loss 2.246\n",
            "Ep 1 (Step 00000472): Train loss 2.063, Val loss 2.316\n",
            "Ep 1 (Step 00000473): Train loss 2.038, Val loss 2.084\n",
            "Ep 1 (Step 00000474): Train loss 2.025, Val loss 2.256\n",
            "Ep 1 (Step 00000475): Train loss 2.370, Val loss 2.176\n",
            "Ep 1 (Step 00000476): Train loss 2.254, Val loss 2.229\n",
            "Ep 1 (Step 00000477): Train loss 2.257, Val loss 2.256\n",
            "Ep 1 (Step 00000478): Train loss 2.137, Val loss 2.403\n",
            "Ep 1 (Step 00000479): Train loss 1.877, Val loss 2.143\n",
            "Ep 1 (Step 00000480): Train loss 2.207, Val loss 2.418\n",
            "Ep 1 (Step 00000481): Train loss 1.926, Val loss 2.090\n",
            "Ep 1 (Step 00000482): Train loss 2.101, Val loss 2.334\n",
            "Ep 1 (Step 00000483): Train loss 2.202, Val loss 2.423\n",
            "Ep 1 (Step 00000484): Train loss 2.457, Val loss 2.134\n",
            "Ep 1 (Step 00000485): Train loss 2.017, Val loss 2.352\n",
            "Ep 1 (Step 00000486): Train loss 2.051, Val loss 2.586\n",
            "Ep 1 (Step 00000487): Train loss 1.878, Val loss 2.471\n",
            "Ep 1 (Step 00000488): Train loss 1.682, Val loss 2.581\n",
            "Ep 1 (Step 00000489): Train loss 2.222, Val loss 2.207\n",
            "Ep 1 (Step 00000490): Train loss 1.927, Val loss 2.333\n",
            "Ep 1 (Step 00000491): Train loss 2.023, Val loss 2.151\n",
            "Ep 1 (Step 00000492): Train loss 1.981, Val loss 2.498\n",
            "Ep 1 (Step 00000493): Train loss 2.028, Val loss 2.275\n",
            "Ep 1 (Step 00000494): Train loss 1.845, Val loss 2.088\n",
            "Ep 1 (Step 00000495): Train loss 1.892, Val loss 2.639\n",
            "Ep 1 (Step 00000496): Train loss 2.050, Val loss 2.460\n",
            "Ep 1 (Step 00000497): Train loss 2.002, Val loss 2.402\n",
            "Ep 1 (Step 00000498): Train loss 1.683, Val loss 2.205\n",
            "Ep 1 (Step 00000499): Train loss 1.973, Val loss 2.195\n",
            "Ep 1 (Step 00000500): Train loss 2.021, Val loss 2.106\n",
            "Ep 1 (Step 00000501): Train loss 1.919, Val loss 2.512\n",
            "Ep 1 (Step 00000502): Train loss 2.312, Val loss 1.927\n",
            "Ep 1 (Step 00000503): Train loss 2.126, Val loss 2.241\n",
            "Ep 1 (Step 00000504): Train loss 1.701, Val loss 2.217\n",
            "Ep 1 (Step 00000505): Train loss 2.009, Val loss 2.099\n",
            "Ep 1 (Step 00000506): Train loss 2.001, Val loss 2.250\n",
            "Ep 1 (Step 00000507): Train loss 1.984, Val loss 2.561\n",
            "Ep 1 (Step 00000508): Train loss 1.968, Val loss 2.061\n",
            "Ep 1 (Step 00000509): Train loss 1.920, Val loss 2.403\n",
            "Ep 1 (Step 00000510): Train loss 1.834, Val loss 2.619\n",
            "Ep 1 (Step 00000511): Train loss 2.158, Val loss 2.318\n",
            "Ep 1 (Step 00000512): Train loss 1.814, Val loss 2.262\n",
            "Ep 1 (Step 00000513): Train loss 2.064, Val loss 2.664\n",
            "Ep 1 (Step 00000514): Train loss 1.884, Val loss 2.042\n",
            "Ep 1 (Step 00000515): Train loss 1.832, Val loss 2.184\n",
            "Ep 1 (Step 00000516): Train loss 2.017, Val loss 1.852\n",
            "Ep 1 (Step 00000517): Train loss 1.986, Val loss 2.639\n",
            "Ep 1 (Step 00000518): Train loss 1.850, Val loss 2.139\n",
            "Ep 1 (Step 00000519): Train loss 1.957, Val loss 2.166\n",
            "Ep 1 (Step 00000520): Train loss 2.018, Val loss 2.491\n",
            "Ep 1 (Step 00000521): Train loss 2.140, Val loss 2.162\n",
            "Ep 1 (Step 00000522): Train loss 2.146, Val loss 2.212\n",
            "Ep 1 (Step 00000523): Train loss 2.021, Val loss 2.066\n",
            "Ep 1 (Step 00000524): Train loss 1.939, Val loss 2.408\n",
            "Ep 1 (Step 00000525): Train loss 1.842, Val loss 2.267\n",
            "Ep 1 (Step 00000526): Train loss 1.869, Val loss 2.328\n",
            "Ep 1 (Step 00000527): Train loss 1.996, Val loss 2.447\n",
            "Ep 1 (Step 00000528): Train loss 2.347, Val loss 2.111\n",
            "Ep 1 (Step 00000529): Train loss 2.301, Val loss 2.042\n",
            "Ep 1 (Step 00000530): Train loss 1.953, Val loss 2.637\n",
            "Ep 1 (Step 00000531): Train loss 2.147, Val loss 2.550\n",
            "Ep 1 (Step 00000532): Train loss 2.089, Val loss 2.275\n",
            "Ep 1 (Step 00000533): Train loss 1.994, Val loss 2.150\n",
            "Ep 1 (Step 00000534): Train loss 1.848, Val loss 2.498\n",
            "Ep 1 (Step 00000535): Train loss 1.949, Val loss 2.153\n",
            "Ep 1 (Step 00000536): Train loss 1.857, Val loss 2.476\n",
            "Ep 1 (Step 00000537): Train loss 2.195, Val loss 2.348\n",
            "Ep 1 (Step 00000538): Train loss 2.055, Val loss 2.435\n",
            "Ep 1 (Step 00000539): Train loss 2.107, Val loss 2.102\n",
            "Ep 1 (Step 00000540): Train loss 1.967, Val loss 2.627\n",
            "Ep 1 (Step 00000541): Train loss 2.010, Val loss 2.204\n",
            "Ep 1 (Step 00000542): Train loss 1.976, Val loss 2.327\n",
            "Ep 1 (Step 00000543): Train loss 2.026, Val loss 2.314\n",
            "Ep 1 (Step 00000544): Train loss 1.924, Val loss 2.200\n",
            "Ep 1 (Step 00000545): Train loss 2.109, Val loss 2.223\n",
            "Ep 1 (Step 00000546): Train loss 1.975, Val loss 1.990\n",
            "Ep 1 (Step 00000547): Train loss 1.844, Val loss 2.338\n",
            "Ep 1 (Step 00000548): Train loss 1.999, Val loss 2.056\n",
            "Ep 1 (Step 00000549): Train loss 2.140, Val loss 2.211\n",
            "Ep 1 (Step 00000550): Train loss 1.893, Val loss 2.366\n",
            "Ep 1 (Step 00000551): Train loss 2.054, Val loss 2.305\n",
            "Ep 1 (Step 00000552): Train loss 1.978, Val loss 1.886\n",
            "Ep 1 (Step 00000553): Train loss 1.917, Val loss 2.061\n",
            "Ep 1 (Step 00000554): Train loss 1.902, Val loss 2.355\n",
            "Ep 1 (Step 00000555): Train loss 2.009, Val loss 2.253\n",
            "Ep 1 (Step 00000556): Train loss 2.045, Val loss 1.979\n",
            "Ep 1 (Step 00000557): Train loss 2.048, Val loss 1.999\n",
            "Ep 1 (Step 00000558): Train loss 1.603, Val loss 2.035\n",
            "Ep 1 (Step 00000559): Train loss 1.991, Val loss 2.411\n",
            "Ep 1 (Step 00000560): Train loss 1.990, Val loss 2.265\n",
            "Ep 1 (Step 00000561): Train loss 1.922, Val loss 2.010\n",
            "Ep 1 (Step 00000562): Train loss 2.366, Val loss 2.276\n",
            "hee\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'generate_text_simple' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-79977d60f7c8>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mtext_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"1 c. firmly packed brown sugar 1/2 c. evaporated milk 1/2 tsp. vanilla 1/2 c. broken nuts (pecans) 2 Tbsp. butter or margarine 3 1/2 c. bite size shredded rice biscuits\\nInstruction: \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m train_losses, val_losses, tokens_seen = train_model_simple(\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-e7d818197bd6>\u001b[0m in \u001b[0;36mtrain_model_simple\u001b[0;34m(model, train_loader, val_loader, optimizer, device, num_epochs, eval_freq, eval_iter, start_context)\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'hee'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;31m# Print a sample text after each epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         generate_and_print_sample(\n\u001b[0m\u001b[1;32m     33\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         )\n",
            "\u001b[0;32m<ipython-input-12-e7d818197bd6>\u001b[0m in \u001b[0;36mgenerate_and_print_sample\u001b[0;34m(model, tokenizer, device, start_context)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0mencoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext_to_token_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         token_ids = generate_text_simple(\n\u001b[0m\u001b[1;32m     58\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoded\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0mmax_new_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcontext_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'generate_text_simple' is not defined"
          ]
        }
      ]
    }
  ]
}